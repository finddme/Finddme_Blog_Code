{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7a7266",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#N-Gram\" data-toc-modified-id=\"N-Gram-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>N-Gram</a></span><ul class=\"toc-item\"><li><span><a href=\"#N-Gram-|-NLTK\" data-toc-modified-id=\"N-Gram-|-NLTK-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>N-Gram | NLTK</a></span></li><li><span><a href=\"#N-Gram-|-TextBlob\" data-toc-modified-id=\"N-Gram-|-TextBlob-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>N-Gram | TextBlob</a></span></li></ul></li><li><span><a href=\"#POS(Part-of-Speech)-tagging\" data-toc-modified-id=\"POS(Part-of-Speech)-tagging-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>POS(Part of Speech) tagging</a></span><ul class=\"toc-item\"><li><span><a href=\"#POS-|-NLTK\" data-toc-modified-id=\"POS-|-NLTK-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>POS | NLTK</a></span></li></ul></li><li><span><a href=\"#불용어(Stop-words)-제거\" data-toc-modified-id=\"불용어(Stop-words)-제거-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>불용어(Stop words) 제거</a></span><ul class=\"toc-item\"><li><span><a href=\"#불용어-제거-|-NLTK\" data-toc-modified-id=\"불용어-제거-|-NLTK-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>불용어 제거 | NLTK</a></span></li></ul></li><li><span><a href=\"#철자교정\" data-toc-modified-id=\"철자교정-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>철자교정</a></span></li><li><span><a href=\"#단수화-복수화\" data-toc-modified-id=\"단수화-복수화-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>단수화 복수화</a></span></li><li><span><a href=\"#Stemming(어간추출)\" data-toc-modified-id=\"Stemming(어간추출)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Stemming(어간추출)</a></span></li><li><span><a href=\"#Lemmatization(표제어-추출)\" data-toc-modified-id=\"Lemmatization(표제어-추출)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Lemmatization(표제어 추출)</a></span></li><li><span><a href=\"#NER(Named-Entity-Recognition,-개채명-인식)\" data-toc-modified-id=\"NER(Named-Entity-Recognition,-개채명-인식)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>NER(Named Entity Recognition, 개채명 인식)</a></span></li><li><span><a href=\"#Lexical-Ambiguity(단어-중의성-해소)\" data-toc-modified-id=\"Lexical-Ambiguity(단어-중의성-해소)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Lexical Ambiguity(단어 중의성 해소)</a></span></li><li><span><a href=\"#BOS(Bag-of-Words)\" data-toc-modified-id=\"BOS(Bag-of-Words)-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>BOS(Bag of Words)</a></span><ul class=\"toc-item\"><li><span><a href=\"#BOS-|-한국어\" data-toc-modified-id=\"BOS-|-한국어-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>BOS | 한국어</a></span></li></ul></li><li><span><a href=\"#DTM(Document-Term-Matrix,-문서-단어-행렬)\" data-toc-modified-id=\"DTM(Document-Term-Matrix,-문서-단어-행렬)-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>DTM(Document Term Matrix, 문서 단어 행렬)</a></span></li><li><span><a href=\"#TF-IDF(Term-Frequency---Inverse-Document-Frequency)\" data-toc-modified-id=\"TF-IDF(Term-Frequency---Inverse-Document-Frequency)-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>TF-IDF(Term Frequency - Inverse Document Frequency)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd7151",
   "metadata": {},
   "source": [
    "## N-Gram\n",
    "\n",
    "토큰을 연속하는 n개로 분류하여 출연 횟수를 추출하는데 쓰인다.\n",
    "\n",
    "n = 1일 때는 unigram, n = 2일 때는 bigram, n = 3일 때는 trigram...\n",
    "\n",
    "bigram이 가장 많이 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c9ffe",
   "metadata": {},
   "source": [
    "### N-Gram | NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56407f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'can'), ('can', 'always'), ('always', 'predict'), ('predict', 'the'), ('the', 'fortune'), ('fortune', 'cookies')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence1 = 'I can always predict the fortune cookies'\n",
    "# n-gram모듈에 sentence1을 split한 것을 넣고, 2개씩 묶어서 나눈다.\n",
    "bigram1 = list(ngrams(sentence1.split(), 2))\n",
    "print(bigram1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee92c343",
   "metadata": {},
   "source": [
    "### N-Gram | TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae3257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'can']),\n",
       " WordList(['can', 'always']),\n",
       " WordList(['always', 'predict']),\n",
       " WordList(['predict', 'the']),\n",
       " WordList(['the', 'fortune']),\n",
       " WordList(['fortune', 'cookies'])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "ngram_blob = TextBlob(sentence1)\n",
    "# TextBlob에 있는 ngrams함수를 사용한다.\n",
    "# n=2로 넣어서 bigram으로 만든다.(default는 trigram으로 설정되어 있다)\n",
    "ngram_blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9740d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'can', 'always']),\n",
       " WordList(['can', 'always', 'predict']),\n",
       " WordList(['always', 'predict', 'the']),\n",
       " WordList(['predict', 'the', 'fortune']),\n",
       " WordList(['the', 'fortune', 'cookies'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_blob.ngrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0f2af",
   "metadata": {},
   "source": [
    "## POS(Part of Speech) tagging\n",
    "\n",
    "POS tagging은 품사태깅이다. 문장 내의 단어들에 대해 품사를 부착해주는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1766f3f",
   "metadata": {},
   "source": [
    "### POS | NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf421a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ac2f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', 'always', 'predict', 'the', 'fortune', 'cookies']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 먼저 tokenize해준다.\n",
    "tokenize_result = word_tokenize(sentence1)\n",
    "tokenize_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9564d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger') # tagger를 다운로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f3b250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('always', 'RB'),\n",
       " ('predict', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('fortune', 'NN'),\n",
       " ('cookies', 'NNS')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenize_result) # nltk의 pos_tag이라는 함수를 사용하여 태깅해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68c78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('always', 'RB'),\n",
       " ('predict', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('fortune', 'NN'),\n",
       " ('cookies', 'NNS')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize와 pos tagging을 한번에\n",
    "nltk.pos_tag(word_tokenize(sentence1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0db611",
   "metadata": {},
   "source": [
    "> POS Tag List(영어기준. 한국어와 맞지 않는 품사도 있음)\n",
    "\n",
    "| Number | Tag | Description | 설명 |\n",
    "| -- | -- | -- | -- |\n",
    "| 1 | `CC` | Coordinating conjunction |\n",
    "| 2 | `CD` | Cardinal number |\n",
    "| 3 | `DT` | Determiner | 한정사\n",
    "| 4 | `EX` | Existential there |\n",
    "| 5 | `FW` | Foreign word | 외래어 |\n",
    "| 6 | `IN` | Preposition or subordinating conjunction | 전치사 또는 종속 접속사 |\n",
    "| 7 | `JJ` | Adjective | 형용사 |\n",
    "| 8 | `JJR` | Adjective, comparative | 헝용사, 비교급 |\n",
    "| 9 | `JJS` | Adjective, superlative | 형용사, 최상급 |\n",
    "| 10 | `LS` | List item marker |\n",
    "| 11 | `MD` | Modal |\n",
    "| 12 | `NN` | Noun, singular or mass | 명사, 단수형 |\n",
    "| 13 | `NNS` | Noun, plural | 명사, 복수형 |\n",
    "| 14 | `NNP` | Proper noun, singular | 고유명사, 단수형 |\n",
    "| 15 | `NNPS` | Proper noun, plural | 고유명사, 복수형 |\n",
    "| 16 | `PDT` | Predeterminer | 전치한정사 |\n",
    "| 17 | `POS` | Possessive ending | 소유형용사 |\n",
    "| 18 | `PRP` | Personal pronoun | 인칭 대명사 |\n",
    "| 19 | `PRP$` | Possessive pronoun | 소유 대명사 |\n",
    "| 20 | `RB` | Adverb | 부사 |\n",
    "| 21 | `RBR` | Adverb, comparative | 부사, 비교급 |\n",
    "| 22 | `RBS` | Adverb, superlative | 부사, 최상급 |\n",
    "| 23 | `RP` | Particle |\n",
    "| 24 | `SYM` | Symbol | 기호\n",
    "| 25 | `TO` | to |\n",
    "| 26 | `UH` | Interjection | 감탄사 |\n",
    "| 27 | `VB` | Verb, base form | 동사, 원형 |\n",
    "| 28 | `VBD` | Verb, past tense | 동사, 과거형 |\n",
    "| 29 | `VBG` | Verb, gerund or present participle | 동사, 현재분사 |\n",
    "| 30 | `VBN` | Verb, past participle | 동사, 과거분사 |\n",
    "| 31 | `VBP` | Verb, non-3rd person singular present | 동사, 비3인칭 단수 |\n",
    "| 32 | `VBZ` | Verb, 3rd person singular present | 동사, 3인칭 단수 |\n",
    "| 33 | `WDT` | Wh-determiner |\n",
    "| 34 | `WP` | Wh-pronoun |\n",
    "| 35 | `WP$` | Possessive wh-pronoun |\n",
    "| 36 | `WRB` | Wh-adverb |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e273e",
   "metadata": {},
   "source": [
    "## 불용어(Stop words) 제거\n",
    "\n",
    "분석에 불필요한 전치사, 접속사, 저빈도 단어 등을 제거해 주면 정확한 분석 결화를 얻을 수 있다.\n",
    "\n",
    "불용어를 제거해주는 도구들이 있지만 데이터에 따라 불용어로 취급되는 것들이 다를 수 있기 때문에 불용어 사전을 직접 만들어 제거해주는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4ab517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'play',\n",
       " 'violin',\n",
       " 'when',\n",
       " \"I'm\",\n",
       " 'thinking',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'I',\n",
       " \"don't\",\n",
       " 'talk',\n",
       " 'for',\n",
       " 'days',\n",
       " 'end.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = \"I play the violin when I'm thinking and sometimes I don't talk for days on end.\"\n",
    "\n",
    "stop_words = \"on the\" # \"on 과 the\"를 불용어로 취급한다\n",
    "# 불용어 사전에 등록한 단어들을 공백을 기준으로 나눠 리스트에 담는다\n",
    "stop_words = stop_words.split(' ') \n",
    "\n",
    "sentence2 = sentence2.split(' ') # 문장을 토큰화한다.\n",
    "\n",
    "except_stop_words = [] # 불용어를 제거한 문장을 담을 리스트를 준비한다.\n",
    "for word in sentence2: # 토큰화된 문장의 단어들을 하나씩 방문해서\n",
    "    if word not in stop_words: # 해당 단어가 불용어 사전에 없으면\n",
    "        except_stop_words.append(word) # 미리 만든 리스트에 넣어라\n",
    "\n",
    "except_stop_words\n",
    "# on과 the가 빠진 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c2225",
   "metadata": {},
   "source": [
    "### 불용어 제거 | NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de25f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk에서 불러온 stopwords라는 불용어 사전의 words를 가져온다. 언어는 영어.\n",
    "stop_words1 = stopwords.words('english') \n",
    "print(stop_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88544ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'play',\n",
       " 'violin',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'thinking',\n",
       " 'sometimes',\n",
       " 'I',\n",
       " \"n't\",\n",
       " 'talk',\n",
       " 'days',\n",
       " 'end',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = \"I play the violin when I'm thinking and sometimes I don't talk for days on end.\"\n",
    "s2_tokenize = word_tokenize(sentence2)\n",
    "except_stop_words1 = []\n",
    "for w in s2_tokenize:\n",
    "    if w not in stop_words1:\n",
    "        except_stop_words1.append(w)\n",
    "except_stop_words1\n",
    "# the, when, and, do, for, on이 불용어로 처리되어 빠졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45f941",
   "metadata": {},
   "source": [
    "## 철자교정\n",
    "\n",
    "텍스트의 오탈자를 고쳐준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fa01167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speller\n"
     ]
    }
   ],
   "source": [
    "# !pip install autocorrect\n",
    "from autocorrect import Speller # Speller 모듈을 불러온다\n",
    "\n",
    "spell1 = Speller('en') # 영어 철자를 검사할 수 있게 객체를 생성한다.\n",
    "\n",
    "print(spell1('spelleerr')) # 철자가 틀린 단어를 넣어본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "649f4386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can always predict the fortune cookies\n"
     ]
    }
   ],
   "source": [
    "wrong_sentence1 = 'I caan alwayss predicta the fortunie cookiess'\n",
    "s1_tokenize = word_tokenize(wrong_sentence1)\n",
    "\n",
    " # s1_tokenize 리스트 안에 있는 단어를 하나씩 돌면서 spell1객체에 통과시켜 철자를 확인하고\n",
    "# 단어 사이에 공백을 넣어 join한다.\n",
    "check_spell = ' '.join([spell1(s) for s in s1_tokenize])\n",
    "print(check_spell)\n",
    "# caan, alwayss, predicta, fortunie, cookiess 다 잘 고쳐졌다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cec1ad",
   "metadata": {},
   "source": [
    "## 단수화 복수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63df19b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', 'always', 'predict', 'the', 'fortune', 'cookies']\n",
      "['I', 'can', 'alway', 'predict', 'the', 'fortune', 'cookie']\n",
      "['we', 'cans', 'alwayss', 'predicts', 'thes', 'fortunes', 'cookiess']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentence1 = 'I can always predict the fortune cookies'\n",
    "textblob1 = TextBlob(sentence1) # 문장을 토큰화한다.\n",
    "print(textblob1.words) # 토큰화 결과\n",
    "print(textblob1.words.singularize()) # 토큰화한 것을 단수화한 결과\n",
    "# always에서 s를 빼버렸다... 복수로 인식했나보다...\n",
    "print(textblob1.words.pluralize())\n",
    "# 그냥 다 s를 붙여버리네"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56c302",
   "metadata": {},
   "source": [
    "## Stemming(어간추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cc0eea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk에서 제공하는 어간추출 라이브러리 중 가장 많이 사용된다.\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "stemmer.stem('stemming') # 어간을 추출해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b40ef",
   "metadata": {},
   "source": [
    "## Lemmatization(표제어 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "363d1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hold'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer1 = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer1.lemmatize('holds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff08fd5",
   "metadata": {},
   "source": [
    "## NER(Named Entity Recognition, 개채명 인식)\n",
    "\n",
    "고유명사와 같은 개체의 이름을 인식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f3ff9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  name/NN\n",
      "  's/POS\n",
      "  (NE Sherlock/NNP Holmes/NNP)\n",
      "  and/CC\n",
      "  the/DT\n",
      "  address/NN\n",
      "  is/VBZ\n",
      "  221B/CD\n",
      "  Baker/NNP\n",
      "  Street/NNP\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('maxent_ne_chunker') # max named entity chunker를 다운받는다\n",
    "nltk.download('words')\n",
    "\n",
    "sentence1 = 'The name\\'s Sherlock Holmes and the address is 221B Baker Street.'\n",
    "\n",
    "s1_pos_tok = nltk.pos_tag(word_tokenize(sentence1)) # tokenize, pos tagging을 해준다\n",
    "\n",
    "s1_named = nltk.ne_chunk(s1_pos_tok, binary=True)\n",
    "print(s1_named)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4bbc9",
   "metadata": {},
   "source": [
    "## Lexical Ambiguity(단어 중의성 해소)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f0017f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.v.07')\n",
      "Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "ambiguous_sentence1 = 'I went to the bank of the river'\n",
    "ambiguous_sentence2 = 'I went to the bank to deposit some money'\n",
    "\n",
    "print(lesk(word_tokenize(ambiguous_sentence1), 'bank')) # 이건 잘 파악하지 못했지만\n",
    "print(lesk(word_tokenize(ambiguous_sentence2), 'bank')) # 얘는 잘 파악했다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de25fd",
   "metadata": {},
   "source": [
    "## BOS(Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8b67fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 2 1 1 1 1]]\n",
      "{'betty': 1, 'bought': 4, 'bit': 2, 'of': 8, 'better': 0, 'butter': 5, 'to': 9, 'make': 7, 'her': 6, 'bitter': 3}\n"
     ]
    }
   ],
   "source": [
    "# 벡터 개수 세는 라이브러리를 불러온다.\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "bos_sentence = ['betty bought a bit of better butter to make her bitter butter better']\n",
    "\n",
    "count_words1 = CountVectorizer() # vector 세어줄 라이브러리를 담은 객체를 생성한다\n",
    "bow1 = count_words1.fit_transform(bos_sentence) # 문장에 fit_transform해준다\n",
    "\n",
    "\n",
    "print(bow1.toarray()) # 벡터 센 결과를 리스트 형태로 출력한다.\n",
    "print(count_words1.vocabulary_) # 결과에서 어떤 단어에 대한 카운트가 어느 인덱스에 있는지 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5999980b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 2 1]]\n",
      "{'betty': 1, 'bought': 4, 'bit': 2, 'better': 0, 'butter': 5, 'make': 6, 'bitter': 3}\n"
     ]
    }
   ],
   "source": [
    "# 불용어는 제거하고 카운트를 셀 수 있다.\n",
    "count_words2 = CountVectorizer(stop_words='english')\n",
    "bow2 = count_words2.fit_transform(bos_sentence)\n",
    "\n",
    "print(bow2.toarray())\n",
    "print(count_words2.vocabulary_)\n",
    "# of, to, her이 불용어로 처리되어 제거됐다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aba5cd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>better</th>\n",
       "      <th>betty</th>\n",
       "      <th>bit</th>\n",
       "      <th>bitter</th>\n",
       "      <th>bought</th>\n",
       "      <th>butter</th>\n",
       "      <th>make</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   better  betty  bit  bitter  bought  butter  make\n",
       "0       2      1    1       1       1       2     1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 보기 편하게 DataFrame으로 바꾸자\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "bos_index = count_words2.vocabulary_\n",
    "bos_result = bow2.toarray()\n",
    "bos_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.)\n",
    "\n",
    "# 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, \n",
    "# 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다.\n",
    "# 이렇게하면 index 순으로 정렬된다.\n",
    "for k, v in sorted(bos_index.items(), key= lambda i : i[1]):\n",
    "    bos_list.append(k) # 리스트에 순서대로 담는다\n",
    "\n",
    "# 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트\n",
    "bos_df = DataFrame(bos_result, columns= bos_list)\n",
    "\n",
    "bos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9975478",
   "metadata": {},
   "source": [
    "### BOS | 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb56e7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1]]\n",
      "{'내가': 2, '어제': 7, '호떡을': 8, '먹고': 3, '싶어서': 6, '건데': 0, '나를': 1, '빼고': 5, '먹었더라고': 4}\n"
     ]
    }
   ],
   "source": [
    "kor_bos_sentence = ['내가 어제 호떡을 먹고 싶어서 사 온 건데 나를 빼고 다 먹었더라고']\n",
    "\n",
    "count_words1 = CountVectorizer()\n",
    "kor_bow = count_words1.fit_transform(kor_bos_sentence)\n",
    "\n",
    "print(kor_bow.toarray())\n",
    "print(count_words1.vocabulary_)\n",
    "# 한국어는 띄어쓰기를 기준으로 나뉜 벡터를 세면 부정확하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76778a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9641c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['내', '가', '어제', '호떡', '을', '먹', '고', '싶', '어서', '사', '온', '건데', '나', '를', '빼', '고', '다', '먹', '었', '더라고']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import MeCab\n",
    "kor_bos_sentence2 = '내가 어제 호떡을 먹고 싶어서 사 온 건데 나를 빼고 다 먹었더라고.'\n",
    "kor_tagger = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\")\n",
    "# 온점은 없애고 형태소 분석한 결과를 kor_tokens에 넣는다\n",
    "kor_tokens = kor_tagger.morphs(re.sub('(\\.)', \"\", kor_bos_sentence2))\n",
    "print(kor_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68db4497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'내': 0, '가': 1, '어제': 2, '호떡': 3, '을': 4, '먹': 5, '고': 6, '싶': 7, '어서': 8, '사': 9, '온': 10, '건데': 11, '나': 12, '를': 13, '빼': 14, '다': 15, '었': 16, '더라고': 17}\n"
     ]
    }
   ],
   "source": [
    "kor_vocab = {} # 토큰화된 단어들을 {토큰:인덱스}형태로 저장할 딕셔너리를 만든다\n",
    "kor_bow2 = [] # bow결과를 저장할 리스트를 만든다\n",
    "\n",
    "for tok in kor_tokens: # 토큰화된 것을 하나씩 돌면서\n",
    "    if tok not in kor_vocab.keys(): # kor_vocab이라는 딕셔너리의 key값에 tok이 없으면\n",
    "         # 딕셔너리 key값으로 tok을 하나 넣고 그의 value값은 딕셔너리의 길이, 즉 원소 개수\n",
    "        kor_vocab[tok] = len(kor_vocab)\n",
    "        # 그리고 리스트 kor_bow에는 '딕셔너리 길이 -1'인덱스 자리에 1을 넣는다\n",
    "        kor_bow2.insert(len(kor_vocab)-1, 1)\n",
    "    else: # kor_vocab이라는 딕셔너리의 key값에 tok이 있으면\n",
    "        index1 = kor_vocab.get(tok) # 딕셔너리에서 key값으로 tok을 가지는 것의 value값을 index1에 넣는다.\n",
    "        # 리스트에서 가져온 인덱스 값에 해당하는 것에 1을 더한다. \n",
    "        kor_bow2[index1] = kor_bow2[index1] + 1\n",
    "print(kor_bow2)\n",
    "print(kor_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d062e030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>내</th>\n",
       "      <th>가</th>\n",
       "      <th>어제</th>\n",
       "      <th>호떡</th>\n",
       "      <th>을</th>\n",
       "      <th>먹</th>\n",
       "      <th>고</th>\n",
       "      <th>싶</th>\n",
       "      <th>어서</th>\n",
       "      <th>사</th>\n",
       "      <th>온</th>\n",
       "      <th>건데</th>\n",
       "      <th>나</th>\n",
       "      <th>를</th>\n",
       "      <th>빼</th>\n",
       "      <th>다</th>\n",
       "      <th>었</th>\n",
       "      <th>더라고</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   내  가  어제  호떡  을  먹  고  싶  어서  사  온  건데  나  를  빼  다  었  더라고\n",
       "0  1  1   1   1  1  2  2  1   1  1  1   1  1  1  1  1  1    1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "kor_bow2_1 = [kor_bow2]\n",
    "\n",
    "kor_bow_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.)\n",
    "\n",
    "# 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, \n",
    "# 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다.\n",
    "# 이렇게하면 index 순으로 정렬된다.\n",
    "for k, v in sorted(kor_vocab.items(), key= lambda i : i[1]):\n",
    "    kor_bow_list.append(k)\n",
    "\n",
    "kor_dtm_df = DataFrame(kor_bow2_1, columns= kor_bow_list)\n",
    "\n",
    "#kor_dtm_df = DataFrame(kor_bow2, index= kor_bow_list)\n",
    "\n",
    "kor_dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66ab96",
   "metadata": {},
   "source": [
    "## DTM(Document Term Matrix, 문서 단어 행렬)\n",
    "\n",
    "문서에 등장하는 단어들의 빈도를 기반으로 행렬을 표현하는 것이다. 즉, 문서에 대한 BOW를 행렬로 표현하는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2008a32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 1 1 1 1 2 1 2 1]]\n",
      "{'crime': 1, 'is': 3, 'common': 0, 'logic': 5, 'rare': 6, 'therefore': 12, 'it': 4, 'upon': 13, 'the': 11, 'rather': 7, 'than': 9, 'that': 10, 'you': 14, 'should': 8, 'dwell': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "doc1 = [\"Crime is common.\", \" Logic is rare.\", \"Therefore it is upon the logic rather than upon the crime that you should dwell.\"]\n",
    "\n",
    "count_words1 = CountVectorizer()\n",
    "doc_bow = count_words1.fit_transform(doc1)\n",
    "\n",
    "print(doc_bow.toarray()) # 각 행은 문서 하나, 열은 해당 문서의 단어를 나타낸다.\n",
    "print(count_words1.vocabulary_) # 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "750aa7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common</th>\n",
       "      <th>crime</th>\n",
       "      <th>dwell</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>logic</th>\n",
       "      <th>rare</th>\n",
       "      <th>rather</th>\n",
       "      <th>should</th>\n",
       "      <th>than</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>therefore</th>\n",
       "      <th>upon</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   common  crime  dwell  is  it  logic  rare  rather  should  than  that  the  \\\n",
       "0       1      1      0   1   0      0     0       0       0     0     0    0   \n",
       "1       0      0      0   1   0      1     1       0       0     0     0    0   \n",
       "2       0      1      1   1   1      1     0       1       1     1     1    2   \n",
       "\n",
       "   therefore  upon  you  \n",
       "0          0     0    0  \n",
       "1          0     0    0  \n",
       "2          1     2    1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 보기 편하게 DataFrame으로 바꾸자\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "index2 = count_words1.vocabulary_\n",
    "bow_result = doc_bow.toarray()\n",
    "dtm_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.)\n",
    "\n",
    "# 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, \n",
    "# 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다.\n",
    "# 이렇게하면 index 순으로 정렬된다.\n",
    "for k, v in sorted(index2.items(), key= lambda i : i[1]):\n",
    "    dtm_list.append(k) # 리스트에 순서대로 담는다\n",
    "\n",
    "# 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트\n",
    "dtm_df = DataFrame(bow_result, columns= dtm_list)\n",
    "\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54b356",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "문서에 등장한 단어를 단순히 count하는 방법이 아니라 다른 문서에 비해 더 많이 나온 단어를 알아보기 위한 방법이다. 다른 문서보다 특정 문서에서 더 많이 등장한 단어가 해당 문서의 핵심어일 가능성이 높다는 가정 하에 만들어진 방법이다.\n",
    "\n",
    "TF-IDF는 Term Frequency * Inverse Document Frequency로 계산된다. Term Frequency (앞서 다룬 DTM이랑 같다. 특정 문서에 나온 단어들의 빈도수), Inverse Document Frequency({전체문서 수}/{특정 단어가 들어있는 문서의 개수 + 1}에 로그를 씌워준 것)\n",
    "\n",
    "scikit-learn에서 tfidfvectorizer를 사용하여 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a78e6954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72033345 0.54783215 0.         0.42544054 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.42544054 0.         0.54783215\n",
      "  0.72033345 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.18177122 0.2390073  0.14116156 0.2390073  0.18177122\n",
      "  0.         0.2390073  0.2390073  0.2390073  0.2390073  0.47801461\n",
      "  0.2390073  0.47801461 0.2390073 ]]\n",
      "{'crime': 1, 'is': 3, 'common': 0, 'logic': 5, 'rare': 6, 'therefore': 12, 'it': 4, 'upon': 13, 'the': 11, 'rather': 7, 'than': 9, 'that': 10, 'you': 14, 'should': 8, 'dwell': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf1 = TfidfVectorizer().fit(doc1) # doc1에 대해 tf-idf를 fit시킨다.\n",
    "\n",
    "print(tfidf1.transform(doc1).toarray())\n",
    "print(tfidf1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e243237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common</th>\n",
       "      <th>crime</th>\n",
       "      <th>dwell</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>logic</th>\n",
       "      <th>rare</th>\n",
       "      <th>rather</th>\n",
       "      <th>should</th>\n",
       "      <th>than</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>therefore</th>\n",
       "      <th>upon</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181771</td>\n",
       "      <td>0.239007</td>\n",
       "      <td>0.141162</td>\n",
       "      <td>0.239007</td>\n",
       "      <td>0.181771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239007</td>\n",
       "      <td>0.239007</td>\n",
       "      <td>0.239007</td>\n",
       "      <td>0.239007</td>\n",
       "      <td>0.478015</td>\n",
       "      <td>0.239007</td>\n",
       "      <td>0.478015</td>\n",
       "      <td>0.239007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     common     crime     dwell        is        it     logic      rare  \\\n",
       "0  0.720333  0.547832  0.000000  0.425441  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.425441  0.000000  0.547832  0.720333   \n",
       "2  0.000000  0.181771  0.239007  0.141162  0.239007  0.181771  0.000000   \n",
       "\n",
       "     rather    should      than      that       the  therefore      upon  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "2  0.239007  0.239007  0.239007  0.239007  0.478015   0.239007  0.478015   \n",
       "\n",
       "        you  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.239007  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "tfidf_index = tfidf1.vocabulary_\n",
    "tfidf_result = tfidf1.transform(doc1).toarray()\n",
    "tfidf_list = [] # column이 될 것들을 추출해서 넣을 리스트를 만든다.(딕셔너리의 key값.)\n",
    "\n",
    "# 딕셔너리를 정렬 시키고 key값으로는 for문을 돌며 들어온 원소의 인덱스 1, \n",
    "# 즉 딕셔너리의 value값을 key로 보고 그것을 기준으로 정렬한다.\n",
    "# 이렇게하면 index 순으로 정렬된다.\n",
    "for k, v in sorted(tfidf_index.items(), key= lambda i : i[1]):\n",
    "    tfidf_list.append(k) # 리스트에 순서대로 담는다\n",
    "\n",
    "# 데이터프레임을 만든다. data는 baw_result, column은 방금 만든 리스트\n",
    "tfidf_df = DataFrame(tfidf_result, columns= tfidf_list)\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176a35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
