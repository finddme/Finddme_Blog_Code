{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cc49ca",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Encoder\" data-toc-modified-id=\"Encoder-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Encoder</a></span></li><li><span><a href=\"#Decoder\" data-toc-modified-id=\"Decoder-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Decoder</a></span></li><li><span><a href=\"#Train(Encoder-+-Decoder)\" data-toc-modified-id=\"Train(Encoder-+-Decoder)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train(Encoder + Decoder)</a></span></li><li><span><a href=\"#Predict\" data-toc-modified-id=\"Predict-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Predict</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a8028",
   "metadata": {},
   "source": [
    "- seq2seq모델은 sequence를 입력 받아 다른 언어 도메인의 sequence를 출력하는 모델이다. 따라서 many-to-many형태의 모델이라고 할 수 있다. 그러나 일반적으로 many-to-many형태의 모데들의 입력과 출력은 동일한 크기를 가지지만 seq2seq은 그렇지 않다는 특징이 있다.\n",
    "- seq2seq모델에는 RNN계열 모델을 활용한 encoder와 decoder가 존재한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582ece3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211b4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib3\n",
    "import zipfile\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccaca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이 실습에서 사용할 데이터는 영-독 데이터이다. 독일어를 영어로 변환해주는 모델을 만들어보자.\n",
    "'''\n",
    "http1 = urllib3.PoolManager()\n",
    "url1 = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
    "filename1 = \"deu-eng.zip\"\n",
    "path1 = os.getcwd() # path는 지금 현재 디렉토리.\n",
    "zipfilename1 = os.path.join(path1, filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f182e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접근해서 http에 request GET방식으로 지정한 url로 가서 읽는 것을 r1이라고 한다.\n",
    "# 그리고 위에서 지정한 zipfilename1을 write binary로 열고 그걸 out_file1이라고 한다.\n",
    "# shell util에서 copyfileobject해서 현재 연 url에서 out_file을 복사해 온다.\n",
    "with http1.request('GET', url1, preload_content=False) as r1, open(zipfilename1, 'wb') as out_file1:\n",
    "    shutil.copyfileobj(r1, out_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가져온게 zipfile이니까 압축을 풀어준다. zipfilename1에 대해 read로 열어주고 zip_ref라고 한다.\n",
    "# 아까 설정한 path에 압축을 풀어준다.\n",
    "with zipfile.ZipFile(zipfilename1, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea054fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨: OS\n",
      " 볼륨 일련 번호: A64A-9C53\n",
      "\n",
      " C:\\Users\\yein4\\seq2seq_test 디렉터리\n",
      "\n",
      "2021-08-22  오후 11:15    <DIR>          .\n",
      "2021-08-22  오후 11:15    <DIR>          ..\n",
      "2021-08-22  오후 09:35    <DIR>          .ipynb_checkpoints\n",
      "2021-08-22  오후 11:16             1,441 _about.txt\n",
      "2021-08-21  오후 02:35            33,336 2021_Attention.ipynb\n",
      "2021-08-22  오후 11:07            37,814 2021_seq2seq(v).ipynb\n",
      "2021-08-22  오후 11:15            36,238 2021_seq2seq(v2).ipynb\n",
      "2021-08-22  오후 11:14            26,822 2021_seq2seq.ipynb\n",
      "2021-08-22  오후 04:49            47,142 2021_seq2seq_with_pytorch.ipynb\n",
      "2021-08-22  오후 11:16        37,686,235 deu.txt\n",
      "2021-08-22  오후 11:16         9,079,830 deu-eng.zip\n",
      "               8개 파일          46,948,858 바이트\n",
      "               3개 디렉터리  270,119,485,440 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "# 디렉토리에 풀린 zip파일을 확인한다.\n",
    "%ls\n",
    "# 잘 풀렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cdcbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dew.txt를 읽어올 것이다. \n",
    "# source language를 독일어로, target language를 영어로 읽어온다.\n",
    "# 그리고 licence 부분이 있는데 이건 나중에 지운다. 일단 있으니까 이름은 지어준다.\n",
    "lines1 = pd.read_csv('deu.txt', names=['tar','src','lic'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67020f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                      tar               src  \\\n",
      "count                              242586            242586   \n",
      "unique                             194650            221045   \n",
      "top     Do you want another one of these?  Es geht mir gut.   \n",
      "freq                                   36                12   \n",
      "\n",
      "                                                      lic  \n",
      "count                                              242586  \n",
      "unique                                             242586  \n",
      "top     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
      "freq                                                    1  \n"
     ]
    }
   ],
   "source": [
    "print(type(lines1))\n",
    "print(lines1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4aba307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      tar               src\n",
      "count                              242586            242586\n",
      "unique                             194650            221045\n",
      "top     Do you want another one of these?  Es geht mir gut.\n",
      "freq                                   36                12\n"
     ]
    }
   ],
   "source": [
    "# lic column을 지운다.\n",
    "del lines1['lic']\n",
    "print(lines1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712bfb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242586"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 양이 얼마나 되는지 확인하자\n",
    "len(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2aec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      tar  \\\n",
      "0                                                     Go.   \n",
      "1                                                     Hi.   \n",
      "2                                                     Hi.   \n",
      "3                                                    Run!   \n",
      "4                                                    Run.   \n",
      "...                                                   ...   \n",
      "242581  If someone who doesn't know your background sa...   \n",
      "242582  If someone who doesn't know your background sa...   \n",
      "242583  It may be impossible to get a completely error...   \n",
      "242584  I know that adding sentences only in your nati...   \n",
      "242585  Doubtless there exists in this world precisely...   \n",
      "\n",
      "                                                      src  \n",
      "0                                                    Geh.  \n",
      "1                                                  Hallo!  \n",
      "2                                              Grüß Gott!  \n",
      "3                                                   Lauf!  \n",
      "4                                                   Lauf!  \n",
      "...                                                   ...  \n",
      "242581  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
      "242582  Wenn jemand, der nicht weiß, woher man kommt, ...  \n",
      "242583  Es ist wohl unmöglich, einen vollkommen fehler...  \n",
      "242584  Ich weiß wohl, dass das ausschließliche Beitra...  \n",
      "242585  Ohne Zweifel findet sich auf dieser Welt zu je...  \n",
      "\n",
      "[242586 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 연습용이니까 데이터를 조금 줄여서 사용한다.\n",
    "lines1 = lines1.loc[:,'tar':'src'] # lines1의 src와 tar를 가져온다.\n",
    "print(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11ebfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = lines1[0:80000] # 그리고 10만개만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ba87d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tar</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tGo.\\n</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tHi.\\n</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tHi.\\n</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tRun!\\n</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tRun.\\n</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\tWow!\\n</td>\n",
       "      <td>Potzdonner!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\tWow!\\n</td>\n",
       "      <td>Donnerwetter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\tDuck!\\n</td>\n",
       "      <td>Kopf runter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\tFire!\\n</td>\n",
       "      <td>Feuer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\tHelp!\\n</td>\n",
       "      <td>Hilfe!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tar            src\n",
       "0    \\tGo.\\n           Geh.\n",
       "1    \\tHi.\\n         Hallo!\n",
       "2    \\tHi.\\n     Grüß Gott!\n",
       "3   \\tRun!\\n          Lauf!\n",
       "4   \\tRun.\\n          Lauf!\n",
       "5   \\tWow!\\n    Potzdonner!\n",
       "6   \\tWow!\\n  Donnerwetter!\n",
       "7  \\tDuck!\\n   Kopf runter!\n",
       "8  \\tFire!\\n         Feuer!\n",
       "9  \\tHelp!\\n         Hilfe!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target문장의 앞과 뒤에 beginning of sequence(\\t) 토큰과 end of sequence(\\n)를 넣어준다.\n",
    "lines1.tar = lines1.tar.apply(lambda x: '\\t' + x + '\\n')\n",
    "lines1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7f28fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy # 토큰화에 사용할 spacy를 install한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1290dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm \n",
    "!python -m spacy download de_core_news_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba44cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'ü', 'û', 'Z', 'l', '?', 'Ö', '4', 'Ü', 'ű', 'H', 'a', 'I', '\\u200b', 'G', '\\u202f', 'ö', '“', '1', 'O', '3', 'L', 't', '%', 'y', '’', 'd', 'á', 'u', ' ', '—', 'r', 'i', 'v', '$', '\\xad', 'Y', 'P', '6', '2', 'z', 'p', 'Ä', '8', '-', 's', 'ū', 'E', 'J', 'n', 'm', 'B', '7', 'ß', '0', 'R', 'C', 'M', ',', 'b', 'f', 'F', 'T', 'h', 'V', 'k', \"'\", 'U', 'q', 'ñ', 'S', 'Q', '9', '„', 'j', ':', '.', 'ä', 'w', 'ˋ', 'g', 'c', '\"', 'W', '5', '–', 'A', '\\xa0', 'D', '+', 'é', 'o', 'X', 'x', '!', 'ō', 'K', 'N']\n",
      "['e', 'Z', 'l', '?', '4', 'H', 'a', '\\t', '/', 'I', 'G', '1', 'O', '3', 'L', 't', '%', 'y', 'd', 'u', ' ', 'r', 'i', 'v', '$', 'Y', 'P', '6', '2', 'z', 'p', '8', '-', 's', 'E', 'J', 'ï', 'n', 'm', 'B', '7', '0', 'R', 'C', 'M', ',', 'b', 'f', 'F', 'T', 'h', 'V', 'k', \"'\", 'q', 'U', 'ñ', 'S', 'Q', '9', 'j', ':', '.', 'w', 'g', 'c', '\"', 'W', '5', 'A', 'D', '+', 'é', '\\n', 'o', 'x', '€', '!', 'K', 'N']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화(tokenization)\n",
    "spacy_de = spacy.load('de_core_news_sm') # 독일어 토큰화(tokenization)\n",
    "\n",
    "# 중복이 없고 순서가 없는 집합 자료형을 만든다.\n",
    "src_vocab = set()\n",
    "tar_vocab = set()\n",
    "\n",
    "\n",
    "for line in lines1.src: \n",
    "    for i in line: \n",
    "        L=spacy_de.tokenizer(i)\n",
    "        src_vocab.add(L.text) \n",
    "\n",
    "\n",
    "for line in lines1.tar: # lines1의 src에서 하나씩 읽어 와서 # tokenize한 걸 L에 담아\n",
    "    for i in line: \n",
    "        L=spacy_en.tokenizer(i)# L에 담긴 단어를 하나씩 방문하여\n",
    "        tar_vocab.add(L.text) \n",
    "        \n",
    "\n",
    "print(list(src_vocab)[:100])\n",
    "print(list(tar_vocab)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2305dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_vocab: [' ', '!', '\"', '$', '%', \"'\", '+', ',', '-', '.']\n",
      "tar_vocab: ['\\t', '\\n', ' ', '!', '\"', '$', '%', \"'\", '+', ',']\n",
      "src_vocab_size: 99\n",
      "tar_vocab_size: 81\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab)) # src_vocab를 리스트로 만들고 정렬한다.\n",
    "tar_vocab = sorted(list(tar_vocab)) # tar_vocab를 리스트로 만들고 정렬한다.\n",
    "\n",
    "src_vocab_size = len(src_vocab) + 1 # src_vocab의 size를 계산한다.\n",
    "tar_vocab_size = len(tar_vocab) +1 # tar_vocab의 size를 계산한다.\n",
    "\n",
    "print(f\"src_vocab: {src_vocab[:10]}\")\n",
    "print(f\"tar_vocab: {tar_vocab[:10]}\")\n",
    "print(f\"src_vocab_size: {src_vocab_size}\")\n",
    "print(f\"tar_vocab_size: {tar_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1330940f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 이제 인덱스를 붙여줘야 한다.\n",
    "# src_vocab과 tar_vocab의 인덱스와 요소값을 하나씩 돌면서 딕셔너리에 넣어주는데, \n",
    "# 인덱스값은 0부터 시작하니까 +1을 해준다.\n",
    "src2idx = dict([(word, i + 1) for i, word in enumerate(src_vocab)])\n",
    "tar2idx = dict([(word, i + 1) for i, word in enumerate(tar_vocab)])\n",
    "\n",
    "# 단어별로 인덱스가 붙어서 딕셔너리를 형성됐다.\n",
    "#print(f\"src2idx: {src2idx}\\n\")\n",
    "#print(f\"tar2idx: {tar2idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb39aeeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input: [[29, 53, 56, 10], [30, 49, 60, 60, 63, 2], [29, 66, 87, 80, 1, 29, 63, 68, 68, 2], [34, 49, 69, 54, 2], [34, 49, 69, 54, 2], [38, 63, 68, 74, 52, 63, 62, 62, 53, 66, 2], [26, 63, 62, 62, 53, 66, 71, 53, 68, 68, 53, 66, 2], [33, 63, 64, 54, 1, 66, 69, 62, 68, 53, 66, 2], [28, 53, 69, 53, 66, 2], [30, 57, 60, 54, 53, 2]]\n",
      "\n",
      "decoder_input: [[1, 32, 65, 12, 2], [1, 33, 59, 12, 2], [1, 33, 59, 12, 2], [1, 43, 71, 64, 4, 2], [1, 43, 71, 64, 12, 2], [1, 48, 65, 73, 4, 2], [1, 48, 65, 73, 4, 2], [1, 29, 71, 53, 61, 4, 2], [1, 31, 59, 68, 55, 4, 2], [1, 33, 55, 62, 66, 4, 2]]\n",
      "\n",
      "decoder_target: [[32, 65, 12, 2], [33, 59, 12, 2], [33, 59, 12, 2], [43, 71, 64, 4, 2], [43, 71, 64, 12, 2], [48, 65, 73, 4, 2], [48, 65, 73, 4, 2], [29, 71, 53, 61, 4, 2], [31, 59, 68, 55, 4, 2], [33, 55, 62, 66, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "이제 인코더에 입력될 입력 데이터를 구성해서 문장 내 단어들을 딕셔너리에서 상응되는\n",
    "인덱스로 변환하여 그걸 리스트로 만든다.\n",
    "source language는 뒤집어서 들어가는 것이 좋다고 논문에 나와있으니까 뒤집어준다.\n",
    "'''\n",
    "# 독일어(Deutsch) 문장을 토큰화한 뒤에 순서를 뒤집는 함수를 만든다\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
    "# 영어(English) 문장을 토큰화 하는 함수를 만든다\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 리스트를 만든다\n",
    "encoder_input = []\n",
    "decoder_input = []\n",
    "decoder_target = []\n",
    "\n",
    "for line in lines1.src:\n",
    "    encoder_input.append([src2idx[word] for word in line])\n",
    "for line in lines1.tar:\n",
    "    decoder_input.append([tar2idx[word] for word in line])\n",
    "for line in lines1.tar:\n",
    "    decoder_target.append([tar2idx[word] for word in line if word != '\\t'])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"encoder_input: {encoder_input[:10]}\\n\")\n",
    "print(f\"decoder_input: {decoder_input[:10]}\\n\")\n",
    "print(f\"decoder_target: {decoder_target[:10]}\")\n",
    "\n",
    "# NotebookApp.iopub_data_rate_limit 해결: https://seong6496.tistory.com/98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ac98e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29 53 56 ...  0  0  0]\n",
      " [30 49 60 ...  0  0  0]\n",
      " [29 66 87 ...  0  0  0]\n",
      " ...\n",
      " [31 51 56 ...  0  0  0]\n",
      " [27 57 62 ...  0  0  0]\n",
      " [31 51 56 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# padding을 해서 길이를 맞춰준다. 데이터에서 가장 긴 길이로 맞춘다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_src_len = max([len(line) for line in lines1.src])\n",
    "max_tar_len = max([len(line) for line in lines1.tar])\n",
    "\n",
    "# padding은 뒷부분에 0을 채우는 방식으로(post) 진행한다.\n",
    "encoder_input = pad_sequences(encoder_input, maxlen= max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen= max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen= max_tar_len, padding='post')\n",
    "\n",
    "print(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2f5c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_categorical을 이용하여 one-hot-vector로 바꿔준다.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60b9b3",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "- encoder는 여러 RNN cell(LSTM)로 구성된다.\n",
    "- encoder로 들어온 단어들을 embedding해서 LSTM을 거친 후 마지막 hidden state를\n",
    "  decoder의 첫 번째 hidden state로 넘겨준다(context vector). 즉, encoder는 \n",
    "  입력 sequence를 고정된 길이의 context vector로 압축해서 decoder에 넘겨주는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f746c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM\n",
    "\n",
    "encoder_input_s = Input(shape=(None, src_vocab_size))\n",
    "# encoder의 LSTM은 latant state를 반환하여 decoder로 넘겨줘야 하기 때문에\n",
    "# return_state를 True로 설정해준다.\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_output_s, hidden_state, cell_state = encoder_lstm(encoder_input_s)\n",
    "encoder_states = [hidden_state, cell_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f3b90",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "- encoder로부터 encoder output과 전체 입력 sequence에 대한 context vector를 받아와서\n",
    "  sequence를 만들어내는 부분이다.\n",
    "- encoder와 마찬가지로 여러 개의 lstm으로 구성된다.\n",
    "- teacher forcing을 위해 정답 데이터가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f8c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "decoder_input_s = Input(shape=(None, tar_vocab_size))\n",
    "# sequence를 반환해야 하니까 return_sequences를 True로 설정하고,\n",
    "# state도 반환하도록 설정한다.\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "# decoder 처음에 들어가는 초기 state값은 encoder에서 받아온 state값을 넣는다\n",
    "decoder_output_s, _, _ = decoder_lstm(decoder_input_s, initial_state=encoder_states)\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "# decoder output을 정의한다. decoder 결과를 softmax에 통과시켜준다.\n",
    "decoder_output_s = decoder_softmax_layer(decoder_output_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc41885",
   "metadata": {},
   "source": [
    "## Train(Encoder + Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9baccb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "500/500 [==============================] - 869s 2s/step - loss: 1.8648 - val_loss: 1.8967\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 877s 2s/step - loss: 1.3198 - val_loss: 1.6912\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 866s 2s/step - loss: 1.1841 - val_loss: 1.5349\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 802s 2s/step - loss: 1.0749 - val_loss: 1.4196\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 755s 2s/step - loss: 1.0072 - val_loss: 1.3638\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 749s 1s/step - loss: 0.9532 - val_loss: 1.2925\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 746s 1s/step - loss: 0.9116 - val_loss: 1.2855\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 746s 1s/step - loss: 0.8727 - val_loss: 1.2265\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 755s 2s/step - loss: 0.8398 - val_loss: 1.1811\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 745s 1s/step - loss: 0.8125 - val_loss: 1.1380\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 747s 1s/step - loss: 0.7847 - val_loss: 1.1965\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 18482s 37s/step - loss: 0.7655 - val_loss: 1.1166\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 832s 2s/step - loss: 0.7437 - val_loss: 1.0580\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 795s 2s/step - loss: 0.7252 - val_loss: 1.0496\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 785s 2s/step - loss: 0.7108 - val_loss: 1.0511\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 791s 2s/step - loss: 0.6935 - val_loss: 1.0401\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 788s 2s/step - loss: 0.6809 - val_loss: 1.0374\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 790s 2s/step - loss: 0.6670 - val_loss: 1.0241\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 788s 2s/step - loss: 0.6585 - val_loss: 1.0057\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 908s 2s/step - loss: 0.6477 - val_loss: 1.0221\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 856s 2s/step - loss: 0.6369 - val_loss: 1.0021\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 835s 2s/step - loss: 0.6331 - val_loss: 0.9920\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 939s 2s/step - loss: 0.6287 - val_loss: 1.0302\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 864s 2s/step - loss: 0.6190 - val_loss: 0.9958\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 651s 1s/step - loss: 0.6059 - val_loss: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14319e77188>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "# encoder에 들어갈 입력과 decoder의 입력을 넣어주고, output을 디코더에서 나온 결과\n",
    "model1 = Model([encoder_input_s, decoder_input_s],decoder_output_s)\n",
    "# model을 compile시킨다. 모델의 파라미터를 설정하고 학습을 진행한다\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# 이제 학습시킨다.\n",
    "model1.fit(x=[encoder_input,decoder_input], \n",
    "           y=decoder_target, \n",
    "           batch_size=128, \n",
    "           epochs=25,\n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ee125",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "- 예측할 때 index 하나씩 예측을 한다. 예측한 인덱스를 저장하고, 다시 입력으로 사용해서 종료 토큰이 나올 때까지 반복한다.\n",
    "- 예측한 index들을 사전을 통해 단어로 변환해서 sequence를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dfa98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs= encoder_input_s, outputs= encoder_states)\n",
    "\n",
    "# 예측에 사용될 decode의 hidden state\n",
    "decoder_hidden_states_input = Input(shape=(256))\n",
    "decoder_cell_states_input = Input(shape=(256))\n",
    "decoder_states_inputs = [decoder_hidden_states_input, decoder_cell_states_input]\n",
    "\n",
    "decoder_output_s, hidden_state, cell_state = decoder_lstm(decoder_input_s, \n",
    "                                                          initial_state=decoder_states_inputs)\n",
    "decoder_states = [hidden_state, cell_state]\n",
    "\n",
    "decoder_output_s = decoder_softmax_layer(decoder_output_s)\n",
    "\n",
    "# 예측 모델\n",
    "decoder_model = Model(inputs=[decoder_input_s] + decoder_states_inputs, \n",
    "                      outputs= [decoder_output_s] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "990a1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index를 단어로 바꿔줄 사전을 만든다.\n",
    "idx2src = dict((i, char) for char, i in src2idx.items())\n",
    "idx2tar = dict((i, char) for char, i in tar2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b6ae7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하는 함수를 정의한다\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # target_seq을 초기화시킨다. 크기는 tar_vocat_size로\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    # tar2idx로 \\t\n",
    "    target_seq[0, 0, tar2idx['\\t']] = 1.\n",
    "    \n",
    "    stop = False # 안 멈추고 계속 반복한다\n",
    "    \n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop:\n",
    "        # encoder model통과하고 나온 값을 target_seq와 함께 디코더에 넣어준다\n",
    "        output_tokens, hidden, cell = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > max_tar_len:\n",
    "            stop = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        states_value = [hidden, cell]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f7ebe8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Halte hier.\n",
      "target: Stop here.\n",
      "translate:  Don't let me do it. \n",
      "\n",
      "input: Warte bis sechs.\n",
      "target: Wait till six.\n",
      "translate:  Why don't you get one? \n",
      "\n",
      "input: Sie können Tom nicht ändern.\n",
      "target: You can't change Tom.\n",
      "translate:  She is a good student. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for seq_idx in [1073, 7538, 50784]:\n",
    "    input_seq = encoder_input[seq_idx : seq_idx+1][::-1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(f\"input: {lines1.src[seq_idx]}\")\n",
    "    print(f\"target: {lines1.tar[seq_idx][1 : len(lines1.tar[seq_idx])-1]}\")\n",
    "    print(\"translate: \", decoded_sentence[:len(decoded_sentence)-1], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
