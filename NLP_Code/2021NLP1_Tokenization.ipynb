{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c555598",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tokenization(토큰화)\" data-toc-modified-id=\"Tokenization(토큰화)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Tokenization(토큰화)</a></span><ul class=\"toc-item\"><li><span><a href=\"#토큰화-|-단어-|-영어\" data-toc-modified-id=\"토큰화-|-단어-|-영어-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>토큰화 | 단어 | 영어</a></span></li><li><span><a href=\"#토큰화-|-단어-|-Mecab\" data-toc-modified-id=\"토큰화-|-단어-|-Mecab-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>토큰화 | 단어 | Mecab</a></span></li><li><span><a href=\"#토큰화-|-단어-|-NLTK\" data-toc-modified-id=\"토큰화-|-단어-|-NLTK-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>토큰화 | 단어 | NLTK</a></span></li><li><span><a href=\"#토큰화-|-문장\" data-toc-modified-id=\"토큰화-|-문장-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>토큰화 | 문장</a></span></li><li><span><a href=\"#토큰화-|-문장-|-NLTK\" data-toc-modified-id=\"토큰화-|-문장-|-NLTK-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>토큰화 | 문장 | NLTK</a></span></li><li><span><a href=\"#토큰화-|-문장-|-kss(Korean-sentence-spliter)\" data-toc-modified-id=\"토큰화-|-문장-|-kss(Korean-sentence-spliter)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>토큰화 | 문장 | kss(Korean sentence spliter)</a></span></li><li><span><a href=\"#토큰화-|-NLTK-|-RegexpTokenizer-|-영어\" data-toc-modified-id=\"토큰화-|-NLTK-|-RegexpTokenizer-|-영어-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>토큰화 | NLTK | RegexpTokenizer | 영어</a></span></li><li><span><a href=\"#토큰화-|-NLTK-|-RegexpTokenizer-|-한국어\" data-toc-modified-id=\"토큰화-|-NLTK-|-RegexpTokenizer-|-한국어-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>토큰화 | NLTK | RegexpTokenizer | 한국어</a></span></li><li><span><a href=\"#토큰화-|-Keras-|-영어\" data-toc-modified-id=\"토큰화-|-Keras-|-영어-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>토큰화 | Keras | 영어</a></span></li><li><span><a href=\"#토큰화-|-Keras-|-한국어\" data-toc-modified-id=\"토큰화-|-Keras-|-한국어-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>토큰화 | Keras | 한국어</a></span></li><li><span><a href=\"#토큰화-|-TextBlob-|-영어\" data-toc-modified-id=\"토큰화-|-TextBlob-|-영어-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>토큰화 | TextBlob | 영어</a></span></li><li><span><a href=\"#토큰화-|-TextBlob-|-한국어\" data-toc-modified-id=\"토큰화-|-TextBlob-|-한국어-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>토큰화 | TextBlob | 한국어</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01b296",
   "metadata": {},
   "source": [
    "## Tokenization(토큰화)\n",
    "\n",
    "토큰화는 말 그대로 문자열을 토큰들로 분리하는 작업이다. 자연어처리에서 토큰화는 중요한 과정이다. 예를 들어 'United Kingdom'을 분리하면 전혀 다른 의미가 되기 때문에 두 단어는 붙여서 처리해야 한다. 즉, 토큰화 대상에서 제외해야 한다. 단어들의 의미를 고려한 토큰화를 지향해야 해당 데이터를 이용한 학습의 결과가 좋아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9a3e4",
   "metadata": {},
   "source": [
    "### 토큰화 | 단어 | 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb056ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'play', 'the', 'violin', 'when', \"I'm\", 'thinking', 'and', 'sometimes', 'I', \"don't\", 'talk', 'for', 'days', 'on', 'end.']\n",
      "<class 'list'>\n",
      "['I', 'play', 'the', 'violin', 'when', \"I'm\", 'thinking', 'and', 'sometimes', 'I', \"don't\", 'talk', 'for', 'days', 'on', 'end.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# sentence1 = 'I can always predict the fortune cookies'\n",
    "sentence1 = \"I play the violin when I'm thinking and sometimes I don't talk for days on end.\"\n",
    "tokens = sentence1.split(' ')\n",
    "print(tokens)\n",
    "print(type(tokens))\n",
    "\n",
    "# sentence1을 공백을 기준으로 split한 결과를 리스트에 담아준다. 이렇게도 가능\n",
    "tokens1 = [x for x in sentence1.split(' ')] \n",
    "print(tokens1)\n",
    "print(type(tokens1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3657ab9",
   "metadata": {},
   "source": [
    "### 토큰화 | 단어 | Mecab\n",
    "\n",
    "한국어는 띄어쓰기를 준수하지 않아도 의미가 전달되기 때문에 띄어쓰기가 지켜지지 않는 경우가 많다. 그래서 공백을 기준으로 토큰화를 하면 데이터에 문제가 생길 수 있다. 그리고 한국어의 형태소는 영어의 형태소와 개념이 다르기 때문에 추가적으로 고려할 사항들이 있다.\n",
    "\n",
    "한국어의 특성을 고려하여 잘 구축된 konlpy라는 라이브러리를 이용하면 처리가 간편하다.형태소분석기는 Mecab을 추천한다. 빠르고 정확하다. 토큰화는 Mecab이라는 형태소분석기를 사용한 결과에서 토큰 결과만 반환받는 설정을 해주면 된다.\n",
    "\n",
    "> 윈도우 Mecab설치 방법:  \n",
    "https://cleancode-ws.tistory.com/97  \n",
    "https://hong-yp-ml-records.tistory.com/91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "052652ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fc45090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('이게', 'NP+JKS'),\n",
       " ('무슨', 'MM'),\n",
       " ('일', 'NNG'),\n",
       " ('이', 'VCP'),\n",
       " ('냐고', 'EC'),\n",
       " ('여름', 'NNG'),\n",
       " ('에', 'JKB'),\n",
       " ('는', 'JX'),\n",
       " ('호떡', 'NNG'),\n",
       " ('을', 'JKO'),\n",
       " ('안', 'MAG'),\n",
       " ('파', 'VV'),\n",
       " ('냐고', 'EC')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MeCab\n",
    "kor_sentence = '이게 무슨 일이냐고 여름에는 호떡을 안 파냐고'\n",
    "kor_tagger = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\") # Mecab을 사용할 객체를 만든다\n",
    "kor_tagger.pos(kor_sentence) # tagger를 통해 pos tagging을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c00839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이게', '무슨', '일', '이', '냐고', '여름', '에', '는', '호떡', '을', '안', '파', '냐고']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 태거를 통해 morph만 뽑아보자(형태소 분석 결과에서 토큰화 결과만 출력.)\n",
    "kor_tagger.morphs(kor_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "067c5bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이게', '일', '여름', '호떡']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사만 추출해보자\n",
    "kor_tagger.nouns(kor_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d80a1",
   "metadata": {},
   "source": [
    "### 토큰화 | 단어 | NLTK\n",
    "\n",
    "NLTK(Natural Language Tool Kit)이라는 패키지의 tokenizer모듈을 활용하여 tokenize해줄 수 있다. 단어 토큰화는 word_tokenize()함수를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9e76f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yein4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # nltk를 불러오고\n",
    "nltk.download('punkt') # nltk에서 토큰화를 위해 punkt를 다운받는다.\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b63c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'play', 'the', 'violin', 'when', 'I', \"'m\", 'thinking', 'and', 'sometimes', 'I', 'do', \"n't\", 'talk', 'for', 'days', 'on', 'end', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize에 문장을 넣어주면 토큰화를 해준다. \n",
    "# 앞서 split을 통해 공백을 기준으로 split한 결과보다 낫다.\n",
    "tokenizer1 = word_tokenize(sentence1)\n",
    "print(tokenizer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8f71b",
   "metadata": {},
   "source": [
    "### 토큰화 | 문장\n",
    "\n",
    "여러 문장들이 있을 때 문장들을 나눠주는 토큰화 방식도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23bd5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quiet. ', ' Calm. ', ' Peaceful. ', \" Isn't it hateful.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences1 = \"Quiet. \\n Calm. \\n Peaceful. \\n Isn't it hateful.\"\n",
    "\n",
    "sentence_tokens = [x for x in sentences1.split('\\n')] # 개행을 기준으로 문장을 split\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10110b",
   "metadata": {},
   "source": [
    "### 토큰화 | 문장 | NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7e15e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개행 표시 있는 문장:  ['Quiet.', 'Calm.', 'Peaceful.', \"Isn't it hateful.\"]\n",
      "개행 표시 없는 문장: ['Quiet.', 'Calm.', 'Peaceful.', \"Isn't it hateful.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize1 = sent_tokenize(sentences1)\n",
    "print(\"개행 표시 있는 문장: \", sent_tokenize1)\n",
    "\n",
    "sentences2 = \"Quiet. Calm. Peaceful. Isn't it hateful.\" # 문장 사이 개행표시 삭제\n",
    "sent_tokenize1 = sent_tokenize(sentences2)\n",
    "print(\"개행 표시 없는 문장:\", sent_tokenize1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae92f9",
   "metadata": {},
   "source": [
    "### 토큰화 | 문장 | kss(Korean sentence spliter)\n",
    "\n",
    "한국어 문장 토큰화는 kss라는 라이브러리를 사용하여 수행한다.(이걸 사용해도 한국어 자연어 처리에는 고려할 사항이 많아서 토큰화가 잘 안된다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d7c2fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-2.6.0-py3-none-any.whl (67 kB)\n",
      "Installing collected packages: kss\n",
      "Successfully installed kss-2.6.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ce569e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이게 무슨 일이냐고 여름에는 호떡을 안 파냐고']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "print(kss.split_sentences(kor_sentence))\n",
    "# 잘 안 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b2310",
   "metadata": {},
   "source": [
    "### 토큰화 | NLTK | RegexpTokenizer | 영어\n",
    "\n",
    "nltk의 tokenize의 RegexpTokenizer(Regular Expression Tokenizer)를 사용하여 토큰화할 수 있다.\n",
    "\n",
    "> 역슬래시(\\\\)를 이용한 정규 표현식 문자 규칙\n",
    "\n",
    "| 문자 | 설명 |\n",
    "| - | - |\n",
    "| `\\\\` | 역슬래시 그자체 |\n",
    "| `\\d` | digit. 모든 숫자(= [0-9]) |\n",
    "| `\\D` | 숫자를 제외한 모든 문자(= [^0-9]) |\n",
    "| `\\s` | space. 공백(= [ \\t\\n\\r\\f\\v]) |\n",
    "| `\\S` | 공백을 제외한 모든 문자(= [^ \\t\\n\\r\\f\\v]) |\n",
    "| `\\w` | word. 문자와 숫자(= [a-zA-Z0-9]) |\n",
    "| `\\W` | 문자와 숫자를 제외한 다른 문자(= [^a-zA-Z0-9]) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a5e077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'play', 'the', 'violin', 'when', 'I', 'm', 'thinking', 'and', 'sometimes', 'I', 'don', 't', 'talk', 'for', 'days', 'on', 'end']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence1 = \"I play the violin when I'm thinking and sometimes I don't talk for days on end.\"\n",
    "\n",
    "# '문자와 숫자가 최소 한개 이상'이라는 패턴에 해당하는 것들만 토큰화한다.\n",
    "# 즉, 문자나 숫자로 된 것만 tokenize하는 tokenizer를 객체에 담아준다.\n",
    "reg_tokenizer1 = RegexpTokenizer(\"[\\w]+\") \n",
    "# 객체를 이용하여 문장을 토큰화한다.\n",
    "tokens1 = reg_tokenizer1.tokenize(sentence1)\n",
    "print(tokens1)\n",
    "# 특수문자를 빼고 처리되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0313e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'play', 'the', 'violin', 'when', 'I', 'm', 'thinking', 'and', 'sometimes', 'I', 'don', 't', 'talk', 'for', 'days', 'on', 'end']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "print(regexp_tokenize(sentence1, \"[\\w]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40936321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'play', 'the', 'violin', 'when', \"I'm\", 'thinking', 'and', 'sometimes', 'I', \"don't\", 'talk', 'for', 'days', 'on', 'end.']\n"
     ]
    }
   ],
   "source": [
    "# gaps=True를 이용하여 해당 정규표현식을 토큰화 기준으로 설정할 수 있다.\n",
    "# 공백을 기준으로 토큰화를 한다.\n",
    "reg_tokenizer2 = RegexpTokenizer(\"[\\s]+\", gaps= True)\n",
    "tokens2 = reg_tokenizer2.tokenize(sentence1)\n",
    "print(tokens2)\n",
    "# 위 코드에서 gaps=True를 없애면 공백만 나온다. 공백만 토큰화하라는 말이 되니까."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae838616",
   "metadata": {},
   "source": [
    "### 토큰화 | NLTK | RegexpTokenizer | 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1cce3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이게', '무슨', '일이냐고', '여름에는', '호떡을', '안', '파냐고']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해당 정규표현식에 걸린 것들만 토큰화한다\n",
    "kor_tokenizer = RegexpTokenizer(\"[가-힣]+\")\n",
    "kor_tokens = kor_tokenizer.tokenize(kor_sentence)\n",
    "kor_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fcd8c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이게', '무슨', '일이냐고', '여름에는', '호떡을', '안', '파냐고']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마찬가지로 gaps=True를 이용하여 해당 정규표현식을 토큰화 기준으로 설정할 수 있다.\n",
    "# 공백을 기준으로 토큰화를 한다.\n",
    "kor_tokenizer2 = RegexpTokenizer(\"[\\s]+\", gaps= True)\n",
    "kor_tokens2 = kor_tokenizer2.tokenize(kor_sentence)\n",
    "kor_tokens2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c543f6",
   "metadata": {},
   "source": [
    "### 토큰화 | Keras | 영어\n",
    "\n",
    "딥러닝 프레임워크인 keras를 이용하여 토큰화를 하는 방법도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9926ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "# text_to_word_sequence라는 모듈에서 tokenizer를 제공한다.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf67ea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'play',\n",
       " 'the',\n",
       " 'violin',\n",
       " 'when',\n",
       " \"i'm\",\n",
       " 'thinking',\n",
       " 'and',\n",
       " 'sometimes',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'talk',\n",
       " 'for',\n",
       " 'days',\n",
       " 'on',\n",
       " 'end']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 공백을 기준으로 토큰화한다.\n",
    "text_to_word_sequence(sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30873714",
   "metadata": {},
   "source": [
    " ### 토큰화 | Keras | 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fdfbb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이게', '무슨', '일이냐고', '여름에는', '호떡을', '안', '파냐고']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(kor_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d879a",
   "metadata": {},
   "source": [
    "### 토큰화 | TextBlob | 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6d17dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a523f55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['I', 'play', 'the', 'violin', 'when', 'I', \"'m\", 'thinking', 'and', 'sometimes', 'I', 'do', \"n't\", 'talk', 'for', 'days', 'on', 'end'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_tokenizer = TextBlob(sentence1)\n",
    "blob_tokenizer.words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3edfb",
   "metadata": {},
   "source": [
    "### 토큰화 | TextBlob | 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd6d8a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['이게', '무슨', '일이냐고', '여름에는', '호떡을', '안', '파냐고'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_blob_tokenizer = TextBlob(kor_sentence)\n",
    "kor_blob_tokenizer.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2befca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
