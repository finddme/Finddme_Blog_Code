{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cc49ca",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Encoder\" data-toc-modified-id=\"Encoder-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Encoder</a></span></li><li><span><a href=\"#Decoder\" data-toc-modified-id=\"Decoder-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Decoder</a></span></li><li><span><a href=\"#Attention\" data-toc-modified-id=\"Attention-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Attention</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train</a></span></li><li><span><a href=\"#Predict\" data-toc-modified-id=\"Predict-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Predict</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a8028",
   "metadata": {},
   "source": [
    "- seq2seq모델은 sequence를 입력 받아 다른 언어 도메인의 sequence를 출력하는 모델이다. 따라서 many-to-many형태의 모델이라고 할 수 있다. 그러나 일반적으로 many-to-many형태의 모데들의 입력과 출력은 동일한 크기를 가지지만 seq2seq은 그렇지 않다는 특징이 있다.\n",
    "- seq2seq모델에는 RNN계열 모델을 활용한 encoder와 decoder가 존재한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582ece3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211b4fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib3\n",
    "import zipfile\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ccaca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이 실습에서 사용할 데이터는 영-독 데이터이다. 독일어를 영어로 변환해주는 모델을 만들어보자.\n",
    "'''\n",
    "http1 = urllib3.PoolManager()\n",
    "url1 = \"http://www.manythings.org/anki/deu-eng.zip\"\n",
    "filename1 = \"deu-eng.zip\"\n",
    "path1 = os.getcwd() # path는 지금 현재 디렉토리.\n",
    "zipfilename1 = os.path.join(path1, filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f182e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 접근해서 http에 request GET방식으로 지정한 url로 가서 읽는 것을 r1이라고 한다.\n",
    "# 그리고 위에서 지정한 zipfilename1을 write binary로 열고 그걸 out_file1이라고 한다.\n",
    "# shell util에서 copyfileobject해서 현재 연 url에서 out_file을 복사해 온다.\n",
    "with http1.request('GET', url1, preload_content=False) as r1, open(zipfilename1, 'wb') as out_file1:\n",
    "    shutil.copyfileobj(r1, out_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가져온게 zipfile이니까 압축을 풀어준다. zipfilename1에 대해 read로 열어주고 zip_ref라고 한다.\n",
    "# 아까 설정한 path에 압축을 풀어준다.\n",
    "with zipfile.ZipFile(zipfilename1, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea054fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨: OS\n",
      " 볼륨 일련 번호: A64A-9C53\n",
      "\n",
      " C:\\Users\\yein4\\seq2seq_test 디렉터리\n",
      "\n",
      "2021-08-23  오전 10:02    <DIR>          .\n",
      "2021-08-23  오전 10:02    <DIR>          ..\n",
      "2021-08-23  오전 10:02    <DIR>          .ipynb_checkpoints\n",
      "2021-08-23  오전 10:04             1,441 _about.txt\n",
      "2021-08-23  오전 10:02            33,336 2021_Attention.ipynb\n",
      "2021-08-22  오후 11:07            37,814 2021_seq2seq(v).ipynb\n",
      "2021-08-23  오전 10:00            36,137 2021_seq2seq(v2).ipynb\n",
      "2021-08-23  오전 10:02            36,241 2021_seq2seq(v2)_Attention.ipynb\n",
      "2021-08-23  오전 10:02            29,907 2021_seq2seq.ipynb\n",
      "2021-08-22  오후 04:49            47,142 2021_seq2seq_with_pytorch.ipynb\n",
      "2021-08-23  오전 10:04        37,686,235 deu.txt\n",
      "2021-08-23  오전 10:04         9,079,830 deu-eng.zip\n",
      "               9개 파일          46,988,083 바이트\n",
      "               3개 디렉터리  266,440,474,624 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "# 디렉토리에 풀린 zip파일을 확인한다.\n",
    "%ls\n",
    "# 잘 풀렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cdcbf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dew.txt를 읽어올 것이다. \n",
    "# source language를 독일어로, target language를 영어로 읽어온다.\n",
    "# 그리고 licence 부분이 있는데 이건 나중에 지운다. 일단 있으니까 이름은 지어준다.\n",
    "lines1 = pd.read_csv('deu.txt', names=['tar','src','lic'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67020f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                      tar               src  \\\n",
      "count                              242586            242586   \n",
      "unique                             194650            221045   \n",
      "top     Do you want another one of these?  Es geht mir gut.   \n",
      "freq                                   36                12   \n",
      "\n",
      "                                                      lic  \n",
      "count                                              242586  \n",
      "unique                                             242586  \n",
      "top     CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
      "freq                                                    1  \n"
     ]
    }
   ],
   "source": [
    "print(type(lines1))\n",
    "print(lines1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4aba307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      tar               src\n",
      "count                              242586            242586\n",
      "unique                             194650            221045\n",
      "top     Do you want another one of these?  Es geht mir gut.\n",
      "freq                                   36                12\n"
     ]
    }
   ],
   "source": [
    "# lic column을 지운다.\n",
    "del lines1['lic']\n",
    "print(lines1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "712bfb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242586"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 양이 얼마나 되는지 확인하자\n",
    "len(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2aec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      tar  \\\n",
      "0                                                     Go.   \n",
      "1                                                     Hi.   \n",
      "2                                                     Hi.   \n",
      "3                                                    Run!   \n",
      "4                                                    Run.   \n",
      "...                                                   ...   \n",
      "242581  If someone who doesn't know your background sa...   \n",
      "242582  If someone who doesn't know your background sa...   \n",
      "242583  It may be impossible to get a completely error...   \n",
      "242584  I know that adding sentences only in your nati...   \n",
      "242585  Doubtless there exists in this world precisely...   \n",
      "\n",
      "                                                      src  \n",
      "0                                                    Geh.  \n",
      "1                                                  Hallo!  \n",
      "2                                              Grüß Gott!  \n",
      "3                                                   Lauf!  \n",
      "4                                                   Lauf!  \n",
      "...                                                   ...  \n",
      "242581  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
      "242582  Wenn jemand, der nicht weiß, woher man kommt, ...  \n",
      "242583  Es ist wohl unmöglich, einen vollkommen fehler...  \n",
      "242584  Ich weiß wohl, dass das ausschließliche Beitra...  \n",
      "242585  Ohne Zweifel findet sich auf dieser Welt zu je...  \n",
      "\n",
      "[242586 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 연습용이니까 데이터를 조금 줄여서 사용한다.\n",
    "lines1 = lines1.loc[:,'tar':'src'] # lines1의 src와 tar를 가져온다.\n",
    "print(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11ebfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines1 = lines1[0:80000] # 그리고 10만개만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ba87d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tar</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tGo.\\n</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tHi.\\n</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tHi.\\n</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tRun!\\n</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tRun.\\n</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\tWow!\\n</td>\n",
       "      <td>Potzdonner!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\tWow!\\n</td>\n",
       "      <td>Donnerwetter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\tDuck!\\n</td>\n",
       "      <td>Kopf runter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\tFire!\\n</td>\n",
       "      <td>Feuer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\tHelp!\\n</td>\n",
       "      <td>Hilfe!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tar            src\n",
       "0    \\tGo.\\n           Geh.\n",
       "1    \\tHi.\\n         Hallo!\n",
       "2    \\tHi.\\n     Grüß Gott!\n",
       "3   \\tRun!\\n          Lauf!\n",
       "4   \\tRun.\\n          Lauf!\n",
       "5   \\tWow!\\n    Potzdonner!\n",
       "6   \\tWow!\\n  Donnerwetter!\n",
       "7  \\tDuck!\\n   Kopf runter!\n",
       "8  \\tFire!\\n         Feuer!\n",
       "9  \\tHelp!\\n         Hilfe!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target문장의 앞과 뒤에 beginning of sequence(\\t) 토큰과 end of sequence(\\n)를 넣어준다.\n",
    "lines1.tar = lines1.tar.apply(lambda x: '\\t' + x + '\\n')\n",
    "lines1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7f28fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy # 토큰화에 사용할 spacy를 install한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1290dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_sm \n",
    "!python -m spacy download de_core_news_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba44cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\xad', 'y', 'F', 'Z', ':', 'i', 'x', '4', 'c', 'b', 'ˋ', \"'\", 'o', 'O', '—', 'G', 'p', '\\u200b', '9', 'Ä', 'e', 'u', 'n', '5', 'Ü', '0', '+', 'ß', 'ö', '%', '“', 'ō', 'K', 'V', 'r', '\"', 'û', 't', 'w', 'L', 'H', 'M', '8', 'R', 'P', 'B', 'T', ' ', 'Ö', 'C', 'D', 'q', '?', '!', '’', 'm', '2', 'á', 'd', 'ű', 'h', 'z', '6', 'é', ',', 'E', '„', 's', 'ñ', 'Y', '\\u202f', '7', '3', 'ä', 'j', 'g', 'l', '–', '\\xa0', 'f', 'k', '-', 'S', 'Q', '.', 'I', 'a', 'U', 'N', 'A', 'W', '$', 'ü', 'X', 'v', 'J', 'ū', '1']\n",
      "['y', 'F', 'Z', ':', 'i', 'x', '4', 'c', 'b', \"'\", 'o', 'O', 'G', 'p', '9', 'e', '\\n', 'u', 'n', '5', '0', '+', '%', 'K', 'V', 'r', '\"', 'w', 't', 'L', 'H', 'M', '8', 'R', 'P', 'B', 'T', ' ', 'C', 'D', 'q', '?', '!', 'm', '2', 'd', 'h', 'z', '6', 'é', ',', 'E', 's', '€', 'ñ', 'Y', '7', '3', 'ï', 'j', 'g', 'l', 'f', 'k', '-', 'S', 'Q', '.', '/', 'I', 'a', 'U', 'N', 'A', 'W', '$', 'v', '\\t', 'J', '1']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm') # 영어 토큰화(tokenization)\n",
    "spacy_de = spacy.load('de_core_news_sm') # 독일어 토큰화(tokenization)\n",
    "\n",
    "# 중복이 없고 순서가 없는 집합 자료형을 만든다.\n",
    "src_vocab = set()\n",
    "tar_vocab = set()\n",
    "\n",
    "\n",
    "for line in lines1.src: \n",
    "    for i in line: \n",
    "        L=spacy_de.tokenizer(i)\n",
    "        src_vocab.add(L.text) \n",
    "\n",
    "\n",
    "for line in lines1.tar: # lines1의 src에서 하나씩 읽어 와서 # tokenize한 걸 L에 담아\n",
    "    for i in line: \n",
    "        L=spacy_en.tokenizer(i)# L에 담긴 단어를 하나씩 방문하여\n",
    "        tar_vocab.add(L.text) \n",
    "        \n",
    "\n",
    "print(list(src_vocab)[:100])\n",
    "print(list(tar_vocab)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2305dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_vocab: [' ', '!', '\"', '$', '%', \"'\", '+', ',', '-', '.']\n",
      "tar_vocab: ['\\t', '\\n', ' ', '!', '\"', '$', '%', \"'\", '+', ',']\n",
      "src_vocab_size: 99\n",
      "tar_vocab_size: 81\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab)) # src_vocab를 리스트로 만들고 정렬한다.\n",
    "tar_vocab = sorted(list(tar_vocab)) # tar_vocab를 리스트로 만들고 정렬한다.\n",
    "\n",
    "src_vocab_size = len(src_vocab) + 1 # src_vocab의 size를 계산한다.\n",
    "tar_vocab_size = len(tar_vocab) +1 # tar_vocab의 size를 계산한다.\n",
    "\n",
    "print(f\"src_vocab: {src_vocab[:10]}\")\n",
    "print(f\"tar_vocab: {tar_vocab[:10]}\")\n",
    "print(f\"src_vocab_size: {src_vocab_size}\")\n",
    "print(f\"tar_vocab_size: {tar_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1330940f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 이제 인덱스를 붙여줘야 한다.\n",
    "# src_vocab과 tar_vocab의 인덱스와 요소값을 하나씩 돌면서 딕셔너리에 넣어주는데, \n",
    "# 인덱스값은 0부터 시작하니까 +1을 해준다.\n",
    "src2idx = dict([(word, i + 1) for i, word in enumerate(src_vocab)])\n",
    "tar2idx = dict([(word, i + 1) for i, word in enumerate(tar_vocab)])\n",
    "\n",
    "# 단어별로 인덱스가 붙어서 딕셔너리를 형성됐다.\n",
    "#print(f\"src2idx: {src2idx}\\n\")\n",
    "#print(f\"tar2idx: {tar2idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb39aeeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input: [[29, 53, 56, 10], [30, 49, 60, 60, 63, 2], [29, 66, 87, 80, 1, 29, 63, 68, 68, 2], [34, 49, 69, 54, 2], [34, 49, 69, 54, 2], [38, 63, 68, 74, 52, 63, 62, 62, 53, 66, 2], [26, 63, 62, 62, 53, 66, 71, 53, 68, 68, 53, 66, 2], [33, 63, 64, 54, 1, 66, 69, 62, 68, 53, 66, 2], [28, 53, 69, 53, 66, 2], [30, 57, 60, 54, 53, 2]]\n",
      "\n",
      "decoder_input: [[1, 32, 65, 12, 2], [1, 33, 59, 12, 2], [1, 33, 59, 12, 2], [1, 43, 71, 64, 4, 2], [1, 43, 71, 64, 12, 2], [1, 48, 65, 73, 4, 2], [1, 48, 65, 73, 4, 2], [1, 29, 71, 53, 61, 4, 2], [1, 31, 59, 68, 55, 4, 2], [1, 33, 55, 62, 66, 4, 2]]\n",
      "\n",
      "decoder_target: [[32, 65, 12, 2], [33, 59, 12, 2], [33, 59, 12, 2], [43, 71, 64, 4, 2], [43, 71, 64, 12, 2], [48, 65, 73, 4, 2], [48, 65, 73, 4, 2], [29, 71, 53, 61, 4, 2], [31, 59, 68, 55, 4, 2], [33, 55, 62, 66, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "이제 인코더에 입력될 입력 데이터를 구성해서 문장 내 단어들을 딕셔너리에서 상응되는\n",
    "인덱스로 변환하여 그걸 리스트로 만든다.\n",
    "source language는 뒤집어서 들어가는 것이 좋다고 논문에 나와있으니까 뒤집어준다.\n",
    "'''\n",
    "# 독일어(Deutsch) 문장을 토큰화한 뒤에 순서를 뒤집는 함수를 만든다\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
    "# 영어(English) 문장을 토큰화 하는 함수를 만든다\n",
    "def tokenize_en(text):\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 리스트를 만든다\n",
    "encoder_input = []\n",
    "decoder_input = []\n",
    "decoder_target = []\n",
    "\n",
    "for line in lines1.src:\n",
    "    encoder_input.append([src2idx[word] for word in line])\n",
    "for line in lines1.tar:\n",
    "    decoder_input.append([tar2idx[word] for word in line])\n",
    "for line in lines1.tar:\n",
    "    decoder_target.append([tar2idx[word] for word in line if word != '\\t'])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"encoder_input: {encoder_input[:10]}\\n\")\n",
    "print(f\"decoder_input: {decoder_input[:10]}\\n\")\n",
    "print(f\"decoder_target: {decoder_target[:10]}\")\n",
    "\n",
    "# NotebookApp.iopub_data_rate_limit 해결: https://seong6496.tistory.com/98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ac98e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29 53 56 ...  0  0  0]\n",
      " [30 49 60 ...  0  0  0]\n",
      " [29 66 87 ...  0  0  0]\n",
      " ...\n",
      " [31 51 56 ...  0  0  0]\n",
      " [27 57 62 ...  0  0  0]\n",
      " [31 51 56 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# padding을 해서 길이를 맞춰준다. 데이터에서 가장 긴 길이로 맞춘다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_src_len = max([len(line) for line in lines1.src])\n",
    "max_tar_len = max([len(line) for line in lines1.tar])\n",
    "\n",
    "# padding은 뒷부분에 0을 채우는 방식으로(post) 진행한다.\n",
    "encoder_input = pad_sequences(encoder_input, maxlen= max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen= max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen= max_tar_len, padding='post')\n",
    "\n",
    "print(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2f5c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_categorical을 이용하여 one-hot-vector로 바꿔준다.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60b9b3",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "- encoder는 여러 RNN cell(LSTM)로 구성된다.\n",
    "- encoder로 들어온 단어들을 embedding해서 LSTM을 거친 후 마지막 hidden state를\n",
    "  decoder의 첫 번째 hidden state로 넘겨준다(context vector). 즉, encoder는 \n",
    "  입력 sequence를 고정된 길이의 context vector로 압축해서 decoder에 넘겨주는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69f746c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM\n",
    "\n",
    "encoder_input_s = Input(shape=(None, src_vocab_size))\n",
    "# encoder의 LSTM은 latant state를 반환하여 decoder로 넘겨줘야 하기 때문에\n",
    "# return_state를 True로 설정해준다.\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_output_s, hidden_state, cell_state = encoder_lstm(encoder_input_s)\n",
    "encoder_states = [hidden_state, cell_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f3b90",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "- encoder로부터 encoder output과 전체 입력 sequence에 대한 context vector를 받아와서\n",
    "  sequence를 만들어내는 부분이다.\n",
    "- encoder와 마찬가지로 여러 개의 lstm으로 구성된다.\n",
    "- teacher forcing을 위해 정답 데이터가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f8c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "decoder_input_s = Input(shape=(None, tar_vocab_size))\n",
    "# sequence를 반환해야 하니까 return_sequences를 True로 설정하고,\n",
    "# state도 반환하도록 설정한다.\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "# decoder 처음에 들어가는 초기 state값은 encoder에서 받아온 state값을 넣는다\n",
    "decoder_output_s, _, _ = decoder_lstm(decoder_input_s, initial_state=encoder_states)\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "# decoder output을 정의한다. decoder 결과를 softmax에 통과시켜준다.\n",
    "decoder_output_s = decoder_softmax_layer(decoder_output_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c51fc",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "seq2seq의 결과가 그렇게 좋지는 않다. context vector가 병목현상 비슷한 것을 야기한다. 즉, 하나의 고정 길이 벡터에 모든 정보를 압축하면 정보가 손실될 수밖에 없다. 이러한 문제를 해결하기 위해 encoder의 정보를 하나로 압축하지 않고 그냥 통으로 참조하여 decoder가 예측하는데에 쓴다. 하지만 입력문장 전체를 단순히 참조하지 않고 참조할 가치가 있는 것에 집중하여 그것을 예측에 사용한다. 이것이 Attention Mechanism의 전반적인 개념이다.\n",
    "\n",
    "과정:\n",
    "\n",
    "1. encoder의 hidden state값과 decoder의 hidden state값을 내정해서 attention score를 계산하고 softmax로 attention distribution을 구한다. attention distribution을 구하면 어디에 attention을 해야 하는지 알게 된다.  \n",
    "2. encoder의 attention가중치와 hidden state를 weighted sum해서 attention 값을 구한다.  \n",
    "3. attention값과 decoder의 t시점 hidden state를 concatenate한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c59df41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM\n",
    "\n",
    "encoder_input_s = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "\n",
    "encoder_output_s, h_state, c_state = encoder_lstm(encoder_input_s)\n",
    "encoder_states = [h_state, c_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78c16333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "decoder_input_s = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_output_s, _, _ = decoder_lstm(decoder_input_s, initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e86a20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "# hidden state와 디코더의 최종 출력을 연결하고 S_에 담는다.\n",
    "# 형태를 맞추기 위해 축을추가해준다.\n",
    "S_ = tf.concat([h_state[:, tf.newaxis, :], decoder_output_s[:, :-1, :]], axis=1)\n",
    "\n",
    "# 이제 Attention layer를 만든다.\n",
    "attention1 = Attention()\n",
    "# 방금 concat한 결과랑 encoder의 output들을 넘겨준다. 이러면 context vector가 만들어진다.\n",
    "context_vector = attention1([S_, encoder_output_s])\n",
    "\n",
    "concat1 = tf.concat([decoder_output_s, context_vector], axis=-1)\n",
    "\n",
    "# distribution을 구한다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "\n",
    "decoder_output_s = decoder_softmax_layer(concat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5f186",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7c7a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "A_model = Model([encoder_input_s, decoder_input_s], decoder_output_s)\n",
    "A_model.compile(optimizer='adam', loss= 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b208094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "500/500 [==============================] - 535s 1s/step - loss: 1.7177 - val_loss: 1.7280\n",
      "Epoch 2/25\n",
      "500/500 [==============================] - 536s 1s/step - loss: 1.2183 - val_loss: 1.5738\n",
      "Epoch 3/25\n",
      "500/500 [==============================] - 525s 1s/step - loss: 1.0842 - val_loss: 1.4406\n",
      "Epoch 4/25\n",
      "500/500 [==============================] - 566s 1s/step - loss: 0.9884 - val_loss: 1.3354\n",
      "Epoch 5/25\n",
      "500/500 [==============================] - 507s 1s/step - loss: 0.9185 - val_loss: 1.2610\n",
      "Epoch 6/25\n",
      "500/500 [==============================] - 519s 1s/step - loss: 0.8748 - val_loss: 2.6678\n",
      "Epoch 7/25\n",
      "500/500 [==============================] - 515s 1s/step - loss: 0.8686 - val_loss: 1.1741\n",
      "Epoch 8/25\n",
      "500/500 [==============================] - 531s 1s/step - loss: 0.8376 - val_loss: 1.1696\n",
      "Epoch 9/25\n",
      "500/500 [==============================] - 567s 1s/step - loss: 0.8001 - val_loss: 1.1158\n",
      "Epoch 10/25\n",
      "500/500 [==============================] - 511s 1s/step - loss: 0.7781 - val_loss: 1.1215\n",
      "Epoch 11/25\n",
      "500/500 [==============================] - 503s 1s/step - loss: 0.7567 - val_loss: 1.0880\n",
      "Epoch 12/25\n",
      "500/500 [==============================] - 511s 1s/step - loss: 0.7258 - val_loss: 1.0769\n",
      "Epoch 13/25\n",
      "500/500 [==============================] - 516s 1s/step - loss: 0.6929 - val_loss: 1.0089\n",
      "Epoch 14/25\n",
      "500/500 [==============================] - 516s 1s/step - loss: 0.6664 - val_loss: 1.0241\n",
      "Epoch 15/25\n",
      "500/500 [==============================] - 484s 968ms/step - loss: 0.6473 - val_loss: 1.0062\n",
      "Epoch 16/25\n",
      "500/500 [==============================] - 531s 1s/step - loss: 0.6324 - val_loss: 1.0216\n",
      "Epoch 17/25\n",
      "500/500 [==============================] - 528s 1s/step - loss: 0.6186 - val_loss: 0.9907\n",
      "Epoch 18/25\n",
      "500/500 [==============================] - 530s 1s/step - loss: 0.6045 - val_loss: 0.9974\n",
      "Epoch 19/25\n",
      "500/500 [==============================] - 534s 1s/step - loss: 0.5914 - val_loss: 1.0205\n",
      "Epoch 20/25\n",
      "500/500 [==============================] - 541s 1s/step - loss: 0.5794 - val_loss: 0.9973\n",
      "Epoch 21/25\n",
      "500/500 [==============================] - 547s 1s/step - loss: 0.5692 - val_loss: 0.9903\n",
      "Epoch 22/25\n",
      "500/500 [==============================] - 385s 770ms/step - loss: 0.5584 - val_loss: 1.0034\n",
      "Epoch 23/25\n",
      "500/500 [==============================] - 482s 964ms/step - loss: 0.5497 - val_loss: 1.0115\n",
      "Epoch 24/25\n",
      "500/500 [==============================] - 546s 1s/step - loss: 0.5394 - val_loss: 1.0061\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 540s 1s/step - loss: 0.5316 - val_loss: 1.0153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22dd4d442c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_model.fit(x=[encoder_input, decoder_input], \n",
    "            y=decoder_target,\n",
    "           batch_size=128,\n",
    "           epochs=25,\n",
    "           validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7a049",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bdcad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_outputs를 outputs에 추가\n",
    "encoder_model = Model(inputs= encoder_input_s, \n",
    "                      outputs= [encoder_output_s ,encoder_states])\n",
    "\n",
    "# 이부분 추가\n",
    "encoder_h_state = Input(shape=(256))\n",
    "encoder_output_s = Input(shape=(256))\n",
    "\n",
    "decoder_h_states_input = Input(shape=(256))\n",
    "decoder_c_states_input = Input(shape=(256))\n",
    "\n",
    "decoder_states_inputs = [decoder_h_states_input, decoder_c_states_input]\n",
    "\n",
    "decoder_output_s, h_state, c_state = decoder_lstm(decoder_input_s,\n",
    "                                                  initial_state=decoder_states_inputs)\n",
    "decoder_states = [h_state, c_state]\n",
    "\n",
    "# 축을 추가해서 concat한다.\n",
    "S_ = tf.concat([encoder_h_state[:, tf.newaxis, :], \n",
    "                decoder_output_s[:, :-1, :]], \n",
    "                axis=1)\n",
    "\n",
    "# attention layer에 S_와 encoder output을 넣는다.\n",
    "context_vector = attention1([S_, encoder_output_s])\n",
    "decoder_concat = tf.concat([decoder_output_s, context_vector], axis=-1)\n",
    "\n",
    "# concat한 거를 softmax에 통과시킨다.\n",
    "decoder_output_s = decoder_softmax_layer(decoder_concat)\n",
    "# encoder_h_state, encoder_output_s추가\n",
    "decoder_model = Model(inputs=[decoder_input_s, encoder_h_state, encoder_output_s] \n",
    "                      + decoder_states_inputs, \n",
    "                      outputs= [decoder_output_s] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a54857b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2src = dict((i, char) for char, i in src2idx.items())\n",
    "idx2tar = dict((i, char) for char, i in tar2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b27dae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_predict_part(input_seq):\n",
    "    outputs_input, states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar2idx['\\t']] = 1\n",
    "    \n",
    "    stop = False\n",
    "    \n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop:\n",
    "        # states_value 첫 번째꺼([0]), outputs_input 추가\n",
    "        output_tokens, hidden, context = decoder_model.predict([target_seq, states_value[0], outputs_input] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2tar[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > max_tar_len:\n",
    "            stop = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        states_value = [hidden, context]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0a666bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  Halte hier.\n",
      "target:  Stop here.\n",
      "translate:  Stop that now. \n",
      "\n",
      "input:  Warte bis sechs.\n",
      "target:  Wait till six.\n",
      "translate:  Wait a second. \n",
      "\n",
      "input:  Sie können Tom nicht ändern.\n",
      "target:  You can't change Tom.\n",
      "translate:  You can't stop Tom. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for seq_idx in [1073, 7538, 50784]:\n",
    "    input_seq = encoder_input[seq_idx : seq_idx+1]\n",
    "    decoded_sentence = decoder_predict_part(input_seq)\n",
    "    \n",
    "    print(\"input: \", lines1.src[seq_idx])\n",
    "    print(\"target: \", lines1.tar[seq_idx][1 : len(lines1.tar[seq_idx])-1])\n",
    "    print(\"translate: \", decoded_sentence[:len(decoded_sentence)-1], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
